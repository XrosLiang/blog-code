{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# plotting params\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 10\n",
    "plt.rcParams['axes.titlesize'] = 10\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "plt.rcParams['ytick.labelsize'] = 8\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['figure.titlesize'] = 12\n",
    "plt.rcParams['figure.figsize'] = (13.0, 6.0)\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "data_dir = './data/'\n",
    "plot_dir = './imgs/'\n",
    "dump_dir = './dump/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = False\n",
    "\n",
    "device = torch.device(\"cuda\" if GPU else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if GPU else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure shuffling is turned off\n",
    "train_loader = DataLoader(\n",
    "    MNIST(data_dir, train=True, download=True,\n",
    "           transform=transforms.Compose([\n",
    "               transforms.ToTensor(),\n",
    "               transforms.Normalize((0.1307,), (0.3081,))\n",
    "           ])),\n",
    "    batch_size=64, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallConv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        out = F.relu(F.max_pool2d(self.conv2(out), 2))\n",
    "        out = out.view(-1, 320)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_gradient(losses, model):\n",
    "    \"\"\"Computes the L2 norm of the gradient of the loss \n",
    "    with respect to the weights and biases of the network.\n",
    "    \n",
    "    Since there's a weight and bias vector associated with \n",
    "    every convolutional and fully-connected layer, the square\n",
    "    root of the sum of the squared gradient norms is returned.\n",
    "    \"\"\"\n",
    "    norms = []\n",
    "    for l in losses:\n",
    "        grad_params = torch.autograd.grad(l, model.parameters(), create_graph=True)\n",
    "        grad_norm = 0\n",
    "        for grad in grad_params:\n",
    "            grad_norm += grad.norm(2).pow(2)\n",
    "        norms.append(grad_norm.sqrt())\n",
    "    return norms\n",
    "\n",
    "def accuracy(predicted, ground_truth):\n",
    "    predicted = torch.max(predicted, 1)[1]\n",
    "    total = len(ground_truth)\n",
    "    correct = (predicted == ground_truth).sum().double()\n",
    "    acc = 100 * (correct / total)\n",
    "    return acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_stats = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        output = model(data)\n",
    "        acc = accuracy(output, target)\n",
    "        \n",
    "        # compute batch loss and gradient norm\n",
    "        losses = F.nll_loss(output, target, reduction='none')\n",
    "        grad_norms = accumulate_gradient(losses, model)\n",
    "        indices = [batch_idx*len(data) + i for i in range(len(data))]\n",
    "        \n",
    "        batch_stats = []\n",
    "        for i, g, l in zip(indices, grad_norms, losses):\n",
    "            batch_stats.append([i, [g, l]])\n",
    "        epoch_stats.append(batch_stats)\n",
    "            \n",
    "        # take average loss and accuracy\n",
    "        loss = losses.mean()\n",
    "        \n",
    "        # backwards pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 25 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAcc: {:.2f}%'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "100. * batch_idx / len(train_loader), loss.item(), acc))\n",
    "\n",
    "    return epoch_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SmallConv().to(device)\n",
    "\n",
    "# relu init\n",
    "for m in model.modules():\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_in')\n",
    "\n",
    "# define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.99)\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "stats = []\n",
    "for epoch in range(1,  num_epochs+1):\n",
    "    stats.append(train(model, device, train_loader, optimizer, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(stats, open(dump_dir + \"statistics.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pickle.load(open(dump_dir + \"statistics.pkl\", \"rb\"))[0]\n",
    "stats = [val for sublist in stats for val in sublist]\n",
    "\n",
    "grad_norms = [l[1][0].item() for l in stats]\n",
    "losses = [l[1][1].item() for l in stats]\n",
    "\n",
    "grad_norms = np.asarray(grad_norms)\n",
    "losses = np.asarray(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = np.cov(grad_norms, losses)\n",
    "std_gn = np.std(grad_norms)\n",
    "std_l = np.std(losses)\n",
    "\n",
    "corr = cov / (std_gn * std_l)\n",
    "\n",
    "print(\"Pearson Correlation Coeff: {}\".format(corr[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pickle.load(open(dump_dir + \"statistics.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_vs_gradnorm(list_stats):\n",
    "    flattened = [val for sublist in list_stats for val in sublist]\n",
    "    sorted_idx = sorted(range(len(flattened)), key=lambda k: flattened[k][1][0])\n",
    "    losses = [flattened[idx][1][1].item() for idx in sorted_idx]\n",
    "    return losses\n",
    "\n",
    "def rolling_window(a, window):\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_losses = np.array(loss_vs_gradnorm(stats[0]))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 6))\n",
    "\n",
    "rolling_mean = np.mean(rolling_window(sorted_losses, 50), 1)\n",
    "rolling_std = np.std(rolling_window(sorted_losses, 50), 1)\n",
    "\n",
    "plt.plot(range(len(rolling_mean)), rolling_mean, alpha=0.98, linewidth=0.9)\n",
    "plt.fill_between(range(len(rolling_std)), rolling_mean-rolling_std, rolling_mean+rolling_std, alpha=0.5)\n",
    "\n",
    "plt.grid()\n",
    "plt.savefig(plot_dir + \"loss_vs_grad.jpg\", format=\"jpg\", dpi=250, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
